{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05.MRC_실습.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83373e5cbc6b4963ab70b3733b2c6c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c754d3ac22c4d6ab34584f4459342bb",
              "IPY_MODEL_c72db84248464121b1cc2cc266addfec",
              "IPY_MODEL_97ad398d0e1844f384c9f855f2ff21a9"
            ],
            "layout": "IPY_MODEL_003f6637428a4ba19e758de6b41a8e70"
          }
        },
        "8c754d3ac22c4d6ab34584f4459342bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35e8e698140c479dab934495446b7d41",
            "placeholder": "​",
            "style": "IPY_MODEL_f37dab7ac257452990799de6e2452238",
            "value": ""
          }
        },
        "c72db84248464121b1cc2cc266addfec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8ffe205a4e2440ea44bd01606ce5374",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d40c8081a464b95bfac4f74a475adf9",
            "value": 0
          }
        },
        "97ad398d0e1844f384c9f855f2ff21a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dad0ea55c1974684999deb3943756fd0",
            "placeholder": "​",
            "style": "IPY_MODEL_66b9d02adcd04558b392bb0f107a55b1",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "003f6637428a4ba19e758de6b41a8e70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35e8e698140c479dab934495446b7d41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f37dab7ac257452990799de6e2452238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8ffe205a4e2440ea44bd01606ce5374": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6d40c8081a464b95bfac4f74a475adf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dad0ea55c1974684999deb3943756fd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66b9d02adcd04558b392bb0f107a55b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f994c6068f0842dc8921ed8760d654f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0f9bbac6e314b8f943f6453f5d180bc",
              "IPY_MODEL_984df1e7a9414cc69e2d199216ff118b",
              "IPY_MODEL_e82327f2d6bc45c9b8a8eba6c0d5bec9"
            ],
            "layout": "IPY_MODEL_ab55b545ea10495193aa8351f7628591"
          }
        },
        "f0f9bbac6e314b8f943f6453f5d180bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_342f1cec10db4908b6714078be808b38",
            "placeholder": "​",
            "style": "IPY_MODEL_51c2feef3b214379992940698bf79a64",
            "value": "Downloading: 100%"
          }
        },
        "984df1e7a9414cc69e2d199216ff118b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25bae026a59a4fd4a41e62d78b0c15e1",
            "max": 995526,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f038651187641119e86c760f76cdb0d",
            "value": 995526
          }
        },
        "e82327f2d6bc45c9b8a8eba6c0d5bec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64744bf751bb40189741289867b19ab6",
            "placeholder": "​",
            "style": "IPY_MODEL_394cfb4768ad4aebb6746f276f20c09e",
            "value": " 996k/996k [00:00&lt;00:00, 2.06MB/s]"
          }
        },
        "ab55b545ea10495193aa8351f7628591": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "342f1cec10db4908b6714078be808b38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c2feef3b214379992940698bf79a64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25bae026a59a4fd4a41e62d78b0c15e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f038651187641119e86c760f76cdb0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64744bf751bb40189741289867b19ab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "394cfb4768ad4aebb6746f276f20c09e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e1fb949accd433382d2741ef61a5f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_264d30522ad748d5ada136acb3cdc8db",
              "IPY_MODEL_6efdba1b261945e0b10265cc8e6dddd2",
              "IPY_MODEL_6f5cdfbbe6084e72a3703820be488c7d"
            ],
            "layout": "IPY_MODEL_d3956dfa3b154d958952ef3c8be81dad"
          }
        },
        "264d30522ad748d5ada136acb3cdc8db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b1d902f9ef448c9a6f9ee940fb3f8d3",
            "placeholder": "​",
            "style": "IPY_MODEL_97e3b1c8914f478dba0adfbc69e2608c",
            "value": "Downloading: 100%"
          }
        },
        "6efdba1b261945e0b10265cc8e6dddd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d664007ed349455db5c2ee4e966786f2",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f97e995f69640ee85e894259c408957",
            "value": 29
          }
        },
        "6f5cdfbbe6084e72a3703820be488c7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a519f39ab55545c0b36cbccd589c671a",
            "placeholder": "​",
            "style": "IPY_MODEL_e164748981d7453d82443e748656a128",
            "value": " 29.0/29.0 [00:00&lt;00:00, 701B/s]"
          }
        },
        "d3956dfa3b154d958952ef3c8be81dad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b1d902f9ef448c9a6f9ee940fb3f8d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97e3b1c8914f478dba0adfbc69e2608c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d664007ed349455db5c2ee4e966786f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f97e995f69640ee85e894259c408957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a519f39ab55545c0b36cbccd589c671a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e164748981d7453d82443e748656a128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "128b434d66fd45aeb49931f3aee94d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4f8c66823be4c7e94ce6ce6b529bdf5",
              "IPY_MODEL_4e2238e765af44cdbba06a015de810a0",
              "IPY_MODEL_541b79a5001446979262759bede8f9d9"
            ],
            "layout": "IPY_MODEL_7212a38025a24cca881a030e2a328789"
          }
        },
        "d4f8c66823be4c7e94ce6ce6b529bdf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1387cb5e60c546bba6387298aca731e2",
            "placeholder": "​",
            "style": "IPY_MODEL_4e62ad7d2b1748048d55f146938d1ee4",
            "value": "Downloading: 100%"
          }
        },
        "4e2238e765af44cdbba06a015de810a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce3d5d7c7f254930a05af8f19d8ab8ab",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c682d384d1a423bbdbd62bb61b21ae5",
            "value": 625
          }
        },
        "541b79a5001446979262759bede8f9d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a70c3e2a97d4baab4ff0f1b4cba59d9",
            "placeholder": "​",
            "style": "IPY_MODEL_336990e050164dd6b0ab3ad68859df57",
            "value": " 625/625 [00:00&lt;00:00, 12.4kB/s]"
          }
        },
        "7212a38025a24cca881a030e2a328789": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1387cb5e60c546bba6387298aca731e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e62ad7d2b1748048d55f146938d1ee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce3d5d7c7f254930a05af8f19d8ab8ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c682d384d1a423bbdbd62bb61b21ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a70c3e2a97d4baab4ff0f1b4cba59d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "336990e050164dd6b0ab3ad68859df57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taRK5uyr1A7c",
        "outputId": "6919873e-296b-4719-e306-750680c0d00b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep 23 05:31:26 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KorQuAD 데이터 다운로드 및 저장"
      ],
      "metadata": {
        "id": "jDfA_OfpkQVy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HXxHL3rn1Qfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9602444-0549-4d72-95fc-b25eeeba4a5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-23 05:31:36--  https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n",
            "Resolving korquad.github.io (korquad.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to korquad.github.io (korquad.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38527475 (37M) [application/json]\n",
            "Saving to: ‘./data/KorQuAD_v1.0_train.json’\n",
            "\n",
            "KorQuAD_v1.0_train. 100%[===================>]  36.74M   154MB/s    in 0.2s    \n",
            "\n",
            "2022-09-23 05:31:36 (154 MB/s) - ‘./data/KorQuAD_v1.0_train.json’ saved [38527475/38527475]\n",
            "\n",
            "--2022-09-23 05:31:36--  https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\n",
            "Resolving korquad.github.io (korquad.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to korquad.github.io (korquad.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3881058 (3.7M) [application/json]\n",
            "Saving to: ‘./data/KorQuAD_v1.0_dev.json’\n",
            "\n",
            "KorQuAD_v1.0_dev.js 100%[===================>]   3.70M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-09-23 05:31:37 (66.4 MB/s) - ‘./data/KorQuAD_v1.0_dev.json’ saved [3881058/3881058]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir ./data\n",
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json -P ./data\n",
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json -P ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 트랜스포머 라이브러리 설치"
      ],
      "metadata": {
        "id": "quH-YM90kMb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Elahy1OF196M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e93ee28-4363-4d5c-8c9a-ead1f68fb491"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 12.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 33.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 38.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.22.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 패키지, 라이브러리, 유틸리티 함수"
      ],
      "metadata": {
        "id": "-4PaMWl-kBXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import logging\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import timeit\n",
        "\n",
        "from collections import defaultdict, Counter, namedtuple, OrderedDict\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from typing import List, Optional, Union, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    BertConfig,\n",
        "    BertModel,\n",
        "    BertPreTrainedModel,\n",
        "    BertTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    WEIGHTS_NAME\n",
        ")\n",
        "\n",
        "from transformers.models.bert import BasicTokenizer\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "def to_list(tensor: torch.tensor) -> List:\n",
        "    return tensor.detach().cpu().tolist()"
      ],
      "metadata": {
        "id": "dRL_G5KpkAlc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "83373e5cbc6b4963ab70b3733b2c6c60",
            "8c754d3ac22c4d6ab34584f4459342bb",
            "c72db84248464121b1cc2cc266addfec",
            "97ad398d0e1844f384c9f855f2ff21a9",
            "003f6637428a4ba19e758de6b41a8e70",
            "35e8e698140c479dab934495446b7d41",
            "f37dab7ac257452990799de6e2452238",
            "f8ffe205a4e2440ea44bd01606ce5374",
            "6d40c8081a464b95bfac4f74a475adf9",
            "dad0ea55c1974684999deb3943756fd0",
            "66b9d02adcd04558b392bb0f107a55b1"
          ]
        },
        "outputId": "27c02a48-72c0-431f-8fc0-29c65b343c5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83373e5cbc6b4963ab70b3733b2c6c60"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 타입 및 구조 확인"
      ],
      "metadata": {
        "id": "H2Vm_hPpVXU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join('./data', 'KorQuAD_v1.0_train.json'), 'r', encoding='utf-8') as fin:\n",
        "    train_data = json.load(fin)\n",
        "\n",
        "\"\"\"\n",
        "[실습]\n",
        "데이터의 타입 및 구조를 확인하시오.\n",
        "\"\"\"\n",
        "print(\"The type of the dataset: {}\\n\".format(type(train_data)))\n",
        "print(\"Keys of the train_data: {}\\n\".format(train_data.keys()))\n",
        "\n",
        "print(\"The type of the train_data['version']: {}\".format(type(train_data['version'])))\n",
        "print(\"The type of the train_data['data']: {}\\n\".format(type(train_data['data'])))\n",
        "\n",
        "print(\"The value of the train_data['version']: {}\\n\".format(train_data['version']))\n",
        "\n",
        "print(\"The number of train_data['data']: {}\".format(len(train_data['data'])))\n",
        "print(\"A value of the train_data['data']:\")\n",
        "print(\"{}\\n\".format(train_data['data'][0]))\n",
        "\n",
        "print(\"Keys of the train_data['data']: {}\\n\".format(train_data['data'][0].keys()))\n",
        "\n",
        "print(\"The type of the train_data['data'][i]['title']: {}\".format(type(train_data['data'][0]['title'])))\n",
        "print(\"The value of the train_data['data'][i]['title']: {}\\n\".format(train_data['data'][0]['title']))\n",
        "\n",
        "print(\"The type of the train_data['data'][i]['paragraphs']: {}\\n\".format(type(train_data['data'][0]['paragraphs'])))\n",
        "print(\"The number of train_data['data'][i]['paragraphs']: {}\".format(len(train_data['data'][0]['paragraphs'])))\n",
        "print(\"A value of the train_data['data'][i]['paragraphs'][i]:\")\n",
        "print(\"{}\\n\".format(train_data['data'][0]['paragraphs'][0]))\n",
        "\n",
        "print(\"Keys of the train_data['data'][i]['paragraphs][i]: {}\\n\".format(train_data['data'][0]['paragraphs'][0].keys()))\n",
        "print(\"The type of the train_data['data'][0]['paragraphs'][0]['context']: {}\".format(type(train_data['data'][0]['paragraphs'][0]['context'])))\n",
        "print(\"The value of the train_data['data'][0]['paragraphs'][0]['context']:\\n{}\\n\".format(train_data['data'][0]['paragraphs'][0]['context']))\n",
        "\n",
        "print(\"The type of the train_data['data'][0]['paragraphs'][0]['qas']: {}\".format(type(train_data['data'][0]['paragraphs'][0]['qas'])))\n",
        "print(\"A value of the train_data['data'][i]['paragraphs'][i]['qas']:\\n{}\\n\".format(train_data['data'][0]['paragraphs'][0]['qas'][0]))\n",
        "\n",
        "print()\n",
        "print(train_data['data'][0]['paragraphs'][0]['context'])\n",
        "print(train_data['data'][0]['paragraphs'][1]['context'])"
      ],
      "metadata": {
        "id": "frWySZ_D2_Tb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de77ba08-153b-450e-acb1-019cf2787fcc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The type of the dataset: <class 'dict'>\n",
            "\n",
            "Keys of the train_data: dict_keys(['version', 'data'])\n",
            "\n",
            "The type of the train_data['version']: <class 'str'>\n",
            "The type of the train_data['data']: <class 'list'>\n",
            "\n",
            "The value of the train_data['version']: KorQuAD_v1.0_train\n",
            "\n",
            "The number of train_data['data']: 1420\n",
            "A value of the train_data['data']:\n",
            "{'paragraphs': [{'qas': [{'answers': [{'text': '교향곡', 'answer_start': 54}], 'id': '6566495-0-0', 'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?'}, {'answers': [{'text': '1악장', 'answer_start': 421}], 'id': '6566495-0-1', 'question': '바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?'}, {'answers': [{'text': '베토벤의 교향곡 9번', 'answer_start': 194}], 'id': '6566495-0-2', 'question': '바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?'}, {'answers': [{'text': '파우스트', 'answer_start': 15}], 'id': '6566518-0-0', 'question': '1839년 바그너가 교향곡의 소재로 쓰려고 했던 책은?'}, {'answers': [{'text': '합창교향곡', 'answer_start': 354}], 'id': '6566518-0-1', 'question': '파우스트 서곡의 라단조 조성이 영향을 받은 베토벤의 곡은?'}, {'answers': [{'text': '1839', 'answer_start': 0}], 'id': '5917067-0-0', 'question': '바그너가 파우스트를 처음으로 읽은 년도는?'}, {'answers': [{'text': '파리', 'answer_start': 410}], 'id': '5917067-0-1', 'question': '바그너가 처음 교향곡 작곡을 한 장소는?'}, {'answers': [{'text': '드레스덴', 'answer_start': 534}], 'id': '5917067-0-2', 'question': '바그너의 1악장의 초연은 어디서 연주되었는가?'}], 'context': '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'}, {'qas': [{'answers': [{'text': '한스 폰 뷜로', 'answer_start': 402}], 'id': '6566495-1-0', 'question': '바그너의 작품을 시인의 피로 쓰여졌다고 극찬한 것은 누구인가?'}, {'answers': [{'text': '리스트', 'answer_start': 23}], 'id': '6566495-1-1', 'question': '잊혀져 있는 파우스트 서곡 1악장을 부활시킨 것은 누구인가?'}, {'answers': [{'text': '20루이의 금', 'answer_start': 345}], 'id': '6566495-1-2', 'question': '바그너는 다시 개정된 총보를 얼마를 받고 팔았는가?'}, {'answers': [{'text': '리스트', 'answer_start': 23}], 'id': '6566518-1-0', 'question': '파우스트 교향곡을 부활시킨 사람은?'}, {'answers': [{'text': '한스 폰 뷜로', 'answer_start': 402}], 'id': '6566518-1-1', 'question': '파우스트 교향곡을 피아노 독주용으로 편곡한 사람은?'}, {'answers': [{'text': '리스트', 'answer_start': 23}], 'id': '5917067-1-0', 'question': '1악장을 부활시켜 연주한 사람은?'}, {'answers': [{'text': '한스 폰 뷜로', 'answer_start': 402}], 'id': '5917067-1-1', 'question': '파우스트 교향곡에 감탄하여 피아노곡으로 편곡한 사람은?'}, {'answers': [{'text': '1840년', 'answer_start': 3}], 'id': '5917067-1-2', 'question': '리스트가 바그너와 알게 된 연도는?'}], 'context': '한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.'}, {'qas': [{'answers': [{'text': '주제, 동기', 'answer_start': 70}], 'id': '6566495-2-0', 'question': '서주에는 무엇이 암시되어 있는가?'}, {'answers': [{'text': '제1바이올린', 'answer_start': 148}], 'id': '6566495-2-1', 'question': '첫부분에는 어떤 악기를 사용해 더욱 명확하게 나타내는가?'}, {'answers': [{'text': '소나타 형식', 'answer_start': 272}], 'id': '6566495-2-2', 'question': '주요부는 어떤 형식으로 되어 있는가?'}, {'answers': [{'text': '저음 주제', 'answer_start': 102}], 'id': '6566518-2-0', 'question': '첫 부분의 주요주제를 암시하는 주제는?'}, {'answers': [{'text': 'D장조', 'answer_start': 409}], 'id': '6566518-2-1', 'question': '제2주제의 축소된 재현부의 조성은?'}, {'answers': [{'text': '4/4박자', 'answer_start': 35}], 'id': '5917067-2-0', 'question': '곡이 시작할때의 박자는?'}, {'answers': [{'text': '고뇌와 갈망 동기, 청춘의 사랑 동기', 'answer_start': 115}], 'id': '5917067-2-1', 'question': '이 곡의 주요 주제는?'}, {'answers': [{'text': 'D장조', 'answer_start': 409}], 'id': '5917067-2-2', 'question': '제 2주제에선 무슨 장조로 재현되는가?'}], 'context': '이 작품은 라단조, Sehr gehalten(아주 신중하게), 4/4박자의 부드러운 서주로 서주로 시작되는데, 여기에는 주요 주제, 동기의 대부분이 암시, 예고되어 있다. 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다. 또한 그것을 이어받는 동기도 중요한 역할을 한다. 여기에 새로운 소재가 더해진 뒤에 새로운 주제도 연주된다. 주요부는 Sehr bewegt(아주 격동적으로), 2/2박자의 자유로운 소나타 형식으로 매우 드라마틱한 구상과 유기적인 구성을 하고 있다. 여기에는 지금까지의 주제나 소재 외에도 오보에에 의한 선율과 제2주제를 떠올리게 하는 부차적인 주제가 더해지는데, 중간부에서는 약보3이 중심이 되고 제2주제는 축소된 재현부에서 D장조로 재현된다. 마지막에는 주요 주제를 회상하면서 조용히 마친다.'}], 'title': '파우스트_서곡'}\n",
            "\n",
            "Keys of the train_data['data']: dict_keys(['paragraphs', 'title'])\n",
            "\n",
            "The type of the train_data['data'][i]['title']: <class 'str'>\n",
            "The value of the train_data['data'][i]['title']: 파우스트_서곡\n",
            "\n",
            "The type of the train_data['data'][i]['paragraphs']: <class 'list'>\n",
            "\n",
            "The number of train_data['data'][i]['paragraphs']: 3\n",
            "A value of the train_data['data'][i]['paragraphs'][i]:\n",
            "{'qas': [{'answers': [{'text': '교향곡', 'answer_start': 54}], 'id': '6566495-0-0', 'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?'}, {'answers': [{'text': '1악장', 'answer_start': 421}], 'id': '6566495-0-1', 'question': '바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?'}, {'answers': [{'text': '베토벤의 교향곡 9번', 'answer_start': 194}], 'id': '6566495-0-2', 'question': '바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?'}, {'answers': [{'text': '파우스트', 'answer_start': 15}], 'id': '6566518-0-0', 'question': '1839년 바그너가 교향곡의 소재로 쓰려고 했던 책은?'}, {'answers': [{'text': '합창교향곡', 'answer_start': 354}], 'id': '6566518-0-1', 'question': '파우스트 서곡의 라단조 조성이 영향을 받은 베토벤의 곡은?'}, {'answers': [{'text': '1839', 'answer_start': 0}], 'id': '5917067-0-0', 'question': '바그너가 파우스트를 처음으로 읽은 년도는?'}, {'answers': [{'text': '파리', 'answer_start': 410}], 'id': '5917067-0-1', 'question': '바그너가 처음 교향곡 작곡을 한 장소는?'}, {'answers': [{'text': '드레스덴', 'answer_start': 534}], 'id': '5917067-0-2', 'question': '바그너의 1악장의 초연은 어디서 연주되었는가?'}], 'context': '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'}\n",
            "\n",
            "Keys of the train_data['data'][i]['paragraphs][i]: dict_keys(['qas', 'context'])\n",
            "\n",
            "The type of the train_data['data'][0]['paragraphs'][0]['context']: <class 'str'>\n",
            "The value of the train_data['data'][0]['paragraphs'][0]['context']:\n",
            "1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n",
            "\n",
            "The type of the train_data['data'][0]['paragraphs'][0]['qas']: <class 'list'>\n",
            "A value of the train_data['data'][i]['paragraphs'][i]['qas']:\n",
            "{'answers': [{'text': '교향곡', 'answer_start': 54}], 'id': '6566495-0-0', 'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?'}\n",
            "\n",
            "\n",
            "1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n",
            "한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리: raw data -> examples -> features"
      ],
      "metadata": {
        "id": "WEK139gIoTr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw 데이터 정형화 (raw data -> examples)\n",
        "## class KorQuADExample\n",
        "> 정형화된 데이터를 저장하는 클래스"
      ],
      "metadata": {
        "id": "JzPnbcZVVS_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KorQuADExample(object):\n",
        "    def __init__(self, qas_id, question_text, context_text, answer_text, start_position_character, title, answers=[], is_impossible=False, ):\n",
        "        self.qas_id = qas_id\n",
        "        self.question_text = question_text\n",
        "        self.context_text = context_text\n",
        "        self.answer_text = answer_text\n",
        "        self.title = title\n",
        "        self.is_impossible = is_impossible\n",
        "        self.answers = answers\n",
        "\n",
        "        self.start_position, self.end_position = 0, 0\n",
        "\n",
        "        doc_tokens = []\n",
        "        char_to_word_offset = []\n",
        "        prev_is_whitespace = True\n",
        "        \"\"\"\n",
        "        [실습]\n",
        "        doc_tokens와 char_to_word_offset 리스트를 구축하시오.\n",
        "\n",
        "        doc_tokens: 어절 리스트\n",
        "        char_to_word_offset: 각 어절의 position\n",
        "\n",
        "        ex. 오늘은 마지막 교육 날이다.\n",
        "        doc_tokens = ['오늘은', '마지막', '교육', '날이다.']\n",
        "        char_to_word_offset = [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
        "        \"\"\"\n",
        "        for c in self.context_text:\n",
        "            if self._is_whitespace(c):\n",
        "                prev_is_whitespace = True\n",
        "            else:\n",
        "                if prev_is_whitespace:\n",
        "                    doc_tokens.append(c)\n",
        "                else:\n",
        "                    doc_tokens[-1] += c\n",
        "                prev_is_whitespace = False\n",
        "            char_to_word_offset.append(len(doc_tokens) - 1)\n",
        "\n",
        "        self.doc_tokens = doc_tokens\n",
        "        self.char_to_word_offset = char_to_word_offset\n",
        "\n",
        "        # Train Case\n",
        "        if start_position_character is not None and not is_impossible:\n",
        "            self.start_position = char_to_word_offset[start_position_character]\n",
        "            self.end_position = char_to_word_offset[min(start_position_character + len(answer_text) - 1, len(char_to_word_offset) - 1)]\n",
        "    \n",
        "    def _is_whitespace(self, c):\n",
        "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "            return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "gyUb6pifU9-R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## class KorQuADPreprocessor\n",
        "> 데이터를 읽고, 정형화하는 클래스"
      ],
      "metadata": {
        "id": "SbE3TUeUWq7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class KorQuADProcessor:\n",
        "\n",
        "    def get_train_examples(self, data_dir, filename):\n",
        "        with open(os.path.join(data_dir, filename), \"r\", encoding=\"utf-8\") as reader:\n",
        "            input_data = json.load(reader)[\"data\"][:5000]\n",
        "\n",
        "        return self._create_examples(input_data, \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir, filename):\n",
        "        with open(os.path.join(data_dir, filename), \"r\", encoding=\"utf-8\") as reader:\n",
        "            input_data = json.load(reader)[\"data\"][:1000]\n",
        "\n",
        "        return self._create_examples(input_data, \"dev\")\n",
        "\n",
        "    def _create_examples(self, input_data, set_type):\n",
        "        is_training = set_type == \"train\"\n",
        "        examples = []\n",
        "        \"\"\"\n",
        "        [실습]\n",
        "        input_data를 KorQuADExample 형태로 정형화하시오.\n",
        "\n",
        "        c.f. is_impossible = input_data['data'][i]]['paragraphs'][j]['qas'][k][is_impossible']\n",
        "        \"\"\"\n",
        "        for entry in tqdm(input_data):\n",
        "            title = entry[\"title\"]\n",
        "            for paragraph in entry[\"paragraphs\"]:\n",
        "                context_text = paragraph[\"context\"]\n",
        "                for qa in paragraph[\"qas\"]:\n",
        "                    qas_id = qa[\"id\"]\n",
        "                    question_text = qa[\"question\"]\n",
        "                    start_position_character = None\n",
        "                    answer_text = None\n",
        "                    answers = []\n",
        "\n",
        "                    is_impossible = qa.get(\"is_impossible\", False)\n",
        "                    # Train, Dev case\n",
        "                    if not is_impossible:\n",
        "                        if is_training:\n",
        "                            answer = qa[\"answers\"][0]\n",
        "                            answer_text = answer[\"text\"]\n",
        "                            start_position_character = answer[\"answer_start\"]\n",
        "                        else:\n",
        "                            answers = qa[\"answers\"]\n",
        "\n",
        "                    example = KorQuADExample(\n",
        "                        qas_id=qas_id,\n",
        "                        question_text=question_text,\n",
        "                        context_text=context_text,\n",
        "                        answer_text=answer_text,\n",
        "                        start_position_character=start_position_character,\n",
        "                        title=title,\n",
        "                        is_impossible=is_impossible,\n",
        "                        answers=answers,\n",
        "                    )\n",
        "                    examples.append(example)\n",
        "        \n",
        "        return examples"
      ],
      "metadata": {
        "id": "IIpNZUORVkwa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실습 결과 출력"
      ],
      "metadata": {
        "id": "lhhfU1nCSp3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join('./data', 'KorQuAD_v1.0_dev.json'), 'r', encoding='utf-8') as fin:\n",
        "    dev_data = json.load(fin)\n",
        "sample_data = [dev_data['data'][0]]\n",
        "\n",
        "exp_preprocessor = KorQuADProcessor()\n",
        "sample_example = exp_preprocessor._create_examples(sample_data, 'train')\n",
        "\n",
        "print(len(sample_example))\n",
        "print(type(sample_example))\n",
        "print()\n",
        "\n",
        "print(sample_example[0].qas_id)\n",
        "print(sample_example[0].question_text)\n",
        "print(sample_example[0].context_text)\n",
        "print(sample_example[0].answer_text)\n",
        "print(sample_example[0].title)\n",
        "print(sample_example[0].answers)\n",
        "print(sample_example[0].start_position)\n",
        "print(sample_example[0].end_position)\n",
        "print(sample_example[0].doc_tokens)\n",
        "print(sample_example[0].char_to_word_offset)"
      ],
      "metadata": {
        "id": "sixr99etpJj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ec09d1-d5ac-43d5-edcd-e9297f60ed7e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "<class 'list'>\n",
            "\n",
            "6548850-0-0\n",
            "임종석이 여의도 농민 폭력 시위를 주도한 혐의로 지명수배 된 날은?\n",
            "1989년 2월 15일 여의도 농민 폭력 시위를 주도한 혐의(폭력행위등처벌에관한법률위반)으로 지명수배되었다. 1989년 3월 12일 서울지방검찰청 공안부는 임종석의 사전구속영장을 발부받았다. 같은 해 6월 30일 평양축전에 임수경을 대표로 파견하여 국가보안법위반 혐의가 추가되었다. 경찰은 12월 18일~20일 사이 서울 경희대학교에서 임종석이 성명 발표를 추진하고 있다는 첩보를 입수했고, 12월 18일 오전 7시 40분 경 가스총과 전자봉으로 무장한 특공조 및 대공과 직원 12명 등 22명의 사복 경찰을 승용차 8대에 나누어 경희대학교에 투입했다. 1989년 12월 18일 오전 8시 15분 경 서울청량리경찰서는 호위 학생 5명과 함께 경희대학교 학생회관 건물 계단을 내려오는 임종석을 발견, 검거해 구속을 집행했다. 임종석은 청량리경찰서에서 약 1시간 동안 조사를 받은 뒤 오전 9시 50분 경 서울 장안동의 서울지방경찰청 공안분실로 인계되었다.\n",
            "1989년 2월 15일\n",
            "임종석\n",
            "[]\n",
            "0\n",
            "2\n",
            "['1989년', '2월', '15일', '여의도', '농민', '폭력', '시위를', '주도한', '혐의(폭력행위등처벌에관한법률위반)으로', '지명수배되었다.', '1989년', '3월', '12일', '서울지방검찰청', '공안부는', '임종석의', '사전구속영장을', '발부받았다.', '같은', '해', '6월', '30일', '평양축전에', '임수경을', '대표로', '파견하여', '국가보안법위반', '혐의가', '추가되었다.', '경찰은', '12월', '18일~20일', '사이', '서울', '경희대학교에서', '임종석이', '성명', '발표를', '추진하고', '있다는', '첩보를', '입수했고,', '12월', '18일', '오전', '7시', '40분', '경', '가스총과', '전자봉으로', '무장한', '특공조', '및', '대공과', '직원', '12명', '등', '22명의', '사복', '경찰을', '승용차', '8대에', '나누어', '경희대학교에', '투입했다.', '1989년', '12월', '18일', '오전', '8시', '15분', '경', '서울청량리경찰서는', '호위', '학생', '5명과', '함께', '경희대학교', '학생회관', '건물', '계단을', '내려오는', '임종석을', '발견,', '검거해', '구속을', '집행했다.', '임종석은', '청량리경찰서에서', '약', '1시간', '동안', '조사를', '받은', '뒤', '오전', '9시', '50분', '경', '서울', '장안동의', '서울지방경찰청', '공안분실로', '인계되었다.']\n",
            "[0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 11, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 19, 19, 20, 20, 20, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 24, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 36, 36, 36, 37, 37, 37, 37, 38, 38, 38, 38, 38, 39, 39, 39, 39, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 43, 43, 43, 43, 44, 44, 44, 45, 45, 45, 46, 46, 46, 46, 47, 47, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 51, 51, 51, 51, 52, 52, 53, 53, 53, 53, 54, 54, 54, 55, 55, 55, 55, 56, 56, 57, 57, 57, 57, 57, 58, 58, 58, 59, 59, 59, 59, 60, 60, 60, 60, 61, 61, 61, 61, 62, 62, 62, 62, 63, 63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66, 67, 67, 67, 67, 68, 68, 68, 69, 69, 69, 70, 70, 70, 70, 71, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 74, 74, 74, 75, 75, 75, 75, 76, 76, 76, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 79, 79, 79, 80, 80, 80, 80, 81, 81, 81, 81, 81, 82, 82, 82, 82, 82, 83, 83, 83, 83, 84, 84, 84, 84, 85, 85, 85, 85, 86, 86, 86, 86, 86, 86, 87, 87, 87, 87, 87, 88, 88, 88, 88, 88, 88, 88, 88, 88, 89, 89, 90, 90, 90, 90, 91, 91, 91, 92, 92, 92, 92, 93, 93, 93, 94, 94, 95, 95, 95, 96, 96, 96, 97, 97, 97, 97, 98, 98, 99, 99, 99, 100, 100, 100, 100, 100, 101, 101, 101, 101, 101, 101, 101, 101, 102, 102, 102, 102, 102, 102, 103, 103, 103, 103, 103, 103]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 전처리 및 텐서 데이터 구축 (examples -> features)"
      ],
      "metadata": {
        "id": "mSq3GlUDme-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def whitespace_tokenize(text):\n",
        "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer, orig_answer_text):\n",
        "    \"\"\"\n",
        "    학습을 위한 start, end position을 wordpiece 단위로 조정하는 함수 # 1989, 년, 2, 월, 15, 일 (0, 2), (0, 5)\n",
        "    \"\"\"\n",
        "    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
        "\n",
        "    for new_start in range(input_start, input_end + 1):\n",
        "        for new_end in range(input_end, new_start - 1, -1):\n",
        "            text_span = \" \".join(doc_tokens[new_start : (new_end + 1)])\n",
        "            if text_span == tok_answer_text:\n",
        "                return (new_start, new_end)\n",
        "\n",
        "    return (input_start, input_end)\n",
        "\n",
        "def _new_check_is_max_context(doc_spans, cur_span_index, position):\n",
        "    best_score = None\n",
        "    best_span_index = None\n",
        "    for (span_index, doc_span) in enumerate(doc_spans):\n",
        "        end = doc_span[\"start\"] + doc_span[\"length\"] - 1\n",
        "        if position < doc_span[\"start\"]:\n",
        "            continue\n",
        "        if position > end:\n",
        "            continue\n",
        "        num_left_context = position - doc_span[\"start\"]\n",
        "        num_right_context = end - position\n",
        "        score = min(num_left_context, num_right_context) + 0.01 * doc_span[\"length\"]\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_span_index = span_index\n",
        "\n",
        "    return cur_span_index == best_span_index"
      ],
      "metadata": {
        "id": "X-eYFdQRmnFW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## class KorQuADFeatures"
      ],
      "metadata": {
        "id": "7Vo6QhbQnbH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KorQuADFeatures:\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        token_type_ids,\n",
        "        cls_index,\n",
        "        p_mask,\n",
        "        example_index,\n",
        "        unique_id,\n",
        "        paragraph_len,\n",
        "        token_is_max_context,\n",
        "        tokens,\n",
        "        token_to_orig_map,\n",
        "        start_position,\n",
        "        end_position,\n",
        "        is_impossible,\n",
        "        qas_id = None,\n",
        "    ):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.cls_index = cls_index\n",
        "        self.p_mask = p_mask\n",
        "\n",
        "        self.example_index = example_index\n",
        "        self.unique_id = unique_id\n",
        "        self.paragraph_len = paragraph_len\n",
        "        self.token_is_max_context = token_is_max_context\n",
        "        self.tokens = tokens\n",
        "        self.token_to_orig_map = token_to_orig_map\n",
        "\n",
        "        self.start_position = start_position\n",
        "        self.end_position = end_position\n",
        "        self.is_impossible = is_impossible\n",
        "        self.qas_id = qas_id"
      ],
      "metadata": {
        "id": "WimorU4Fnee3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def KorQuAD_convert_example_to_features"
      ],
      "metadata": {
        "id": "WoMGhzR1nNDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def KorQuAD_convert_example_to_features(example, max_seq_length, doc_stride, max_query_length, is_training):\n",
        "    features = []\n",
        "    if is_training and not example.is_impossible:\n",
        "        # Get start and end position\n",
        "        start_position = example.start_position\n",
        "        end_position = example.end_position\n",
        "\n",
        "        # If the answer cannot be found in the text, then skip this example.\n",
        "        actual_text = \" \".join(example.doc_tokens[start_position : (end_position + 1)])\n",
        "        cleaned_answer_text = \" \".join(whitespace_tokenize(example.answer_text))\n",
        "        if actual_text.find(cleaned_answer_text) == -1:\n",
        "            logger.warning(f\"Could not find answer: '{actual_text}' vs. '{cleaned_answer_text}'\")\n",
        "            return []\n",
        "\n",
        "    tok_to_orig_index = []\n",
        "    orig_to_tok_index = []\n",
        "    all_doc_tokens = []\n",
        "    \"\"\"\n",
        "    [실습]\n",
        "    all_doc_tokens, tok_to_orig_index, orig_to_tok_index 리스트를 구축하고,\n",
        "    실제 정답과 어절 단위 정답이 불일치할 경우, _improve_answer_span 함수를 이용해\n",
        "    wordpiece 단위의 start, end position을 구하시오.\n",
        "\n",
        "    c.f.\n",
        "    all_doc_tokens: Bert Tokenizer를 이용해 wordpiece 단위의 토큰 리스트\n",
        "    tok_to_orig_index: wordpiece 단위 토큰의 어절 단위 인덱스 값\n",
        "    orig_to_tok_index: 각 어절의 wordpiece 시작 인덱스 값\n",
        "\n",
        "    ex. context = \"1989년 2월 15일 여의도 농민 폭력 시위를 주도한 혐의\"\n",
        "    all_doc_tokens = ['1989년', '2월', '15일', '여', '##의', '##도', '농', '##민', '폭', '##력', '시', '##위를', '주', '##도', '##한', '혐', '##의']\n",
        "    tok_to_orig_index = [0, 1, 2, 3, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 7, 8, 8]\n",
        "    orig_to_tok_index = [0, 1, 2, 3, 6, 8, 10, 12, 15]\n",
        "    \"\"\"\n",
        "    for i, token in enumerate(example.doc_tokens):\n",
        "        orig_to_tok_index.append(len(all_doc_tokens))\n",
        "        sub_tokens = tokenizer.tokenize(token)\n",
        "        for sub_token in sub_tokens:\n",
        "            tok_to_orig_index.append(i)\n",
        "            all_doc_tokens.append(sub_token)\n",
        "\n",
        "    if is_training and not example.is_impossible:\n",
        "        tok_start_position = orig_to_tok_index[example.start_position]\n",
        "        if example.end_position < len(example.doc_tokens) - 1:\n",
        "            tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
        "        else:\n",
        "            tok_end_position = len(all_doc_tokens) - 1\n",
        "        (tok_start_position, tok_end_position) = _improve_answer_span(\n",
        "            all_doc_tokens, tok_start_position, tok_end_position, tokenizer, example.answer_text\n",
        "        )\n",
        "    \n",
        "    \"\"\"\n",
        "    c.f.\n",
        "    BERT 입력 길이만큼 입력 시퀀스를 자르고,\n",
        "    입력 시퀀스 중 context 내에 정답이 있는 경우 그대로 사용,\n",
        "    입력 시퀀스 내에 정답이 없는 경우 doc_stride 만큼 입력 시퀀스의 context 조정하는 코드\n",
        "    \"\"\"\n",
        "    spans = []\n",
        "    truncated_query = tokenizer.encode(example.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)\n",
        "\n",
        "    sequence_added_tokens = 2 # [CLS] context [SEP]\n",
        "    sequence_pair_added_tokens = 3 # [CLS] context1 [SEP] context2 [SEP]\n",
        "\n",
        "    span_doc_tokens = all_doc_tokens\n",
        "    while len(spans) * doc_stride < len(all_doc_tokens):\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            truncated_query,\n",
        "            span_doc_tokens,\n",
        "            truncation=\"only_second\",\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_seq_length,\n",
        "            return_overflowing_tokens=True,\n",
        "            stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens,\n",
        "            return_token_type_ids=True,\n",
        "        )\n",
        "\n",
        "        paragraph_len = min(\n",
        "            len(all_doc_tokens) - len(spans) * doc_stride,\n",
        "            max_seq_length - len(truncated_query) - sequence_pair_added_tokens,\n",
        "        )\n",
        "\n",
        "        if tokenizer.pad_token_id in encoded_dict[\"input_ids\"]:\n",
        "            non_padded_ids = encoded_dict[\"input_ids\"][: encoded_dict[\"input_ids\"].index(tokenizer.pad_token_id)]\n",
        "        else:\n",
        "            non_padded_ids = encoded_dict[\"input_ids\"]\n",
        "\n",
        "        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)\n",
        "\n",
        "        token_to_orig_map = {}\n",
        "        for i in range(paragraph_len):\n",
        "            index = len(truncated_query) + sequence_added_tokens + i\n",
        "            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]\n",
        "\n",
        "        encoded_dict[\"paragraph_len\"] = paragraph_len\n",
        "        encoded_dict[\"tokens\"] = tokens\n",
        "        encoded_dict[\"token_to_orig_map\"] = token_to_orig_map\n",
        "        encoded_dict[\"truncated_query_with_special_tokens_length\"] = len(truncated_query) + sequence_added_tokens\n",
        "        encoded_dict[\"token_is_max_context\"] = {}\n",
        "        encoded_dict[\"start\"] = len(spans) * doc_stride\n",
        "        encoded_dict[\"length\"] = paragraph_len\n",
        "\n",
        "        spans.append(encoded_dict)\n",
        "\n",
        "        if \"overflowing_tokens\" not in encoded_dict or (\n",
        "            \"overflowing_tokens\" in encoded_dict and len(encoded_dict[\"overflowing_tokens\"]) == 0\n",
        "        ):\n",
        "            break\n",
        "        span_doc_tokens = encoded_dict[\"overflowing_tokens\"]\n",
        "\n",
        "\n",
        "    for doc_span_index in range(len(spans)):\n",
        "        for j in range(spans[doc_span_index][\"paragraph_len\"]):\n",
        "            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)\n",
        "            index = spans[doc_span_index][\"truncated_query_with_special_tokens_length\"] + j\n",
        "            spans[doc_span_index][\"token_is_max_context\"][index] = is_max_context\n",
        "\n",
        "    for span in spans:\n",
        "        cls_index = span[\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "\n",
        "        p_mask = np.ones_like(span[\"token_type_ids\"])\n",
        "        p_mask[len(truncated_query) + sequence_added_tokens :] = 0\n",
        "\n",
        "        pad_token_indices = np.where(span[\"input_ids\"] == tokenizer.pad_token_id)\n",
        "        special_token_indices = np.asarray(\n",
        "            tokenizer.get_special_tokens_mask(span[\"input_ids\"], already_has_special_tokens=True)\n",
        "        ).nonzero()\n",
        "\n",
        "        p_mask[pad_token_indices] = 1\n",
        "        p_mask[special_token_indices] = 1\n",
        "        p_mask[cls_index] = 0\n",
        "\n",
        "        span_is_impossible = example.is_impossible\n",
        "        start_position = 0\n",
        "        end_position = 0\n",
        "        if is_training and not span_is_impossible:\n",
        "            doc_start = span[\"start\"]\n",
        "            doc_end = span[\"start\"] + span[\"length\"] - 1\n",
        "            out_of_span = False\n",
        "\n",
        "            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):\n",
        "                out_of_span = True\n",
        "\n",
        "            if out_of_span:\n",
        "                start_position = cls_index\n",
        "                end_position = cls_index\n",
        "                span_is_impossible = True\n",
        "            else:\n",
        "                doc_offset = len(truncated_query) + sequence_added_tokens\n",
        "\n",
        "                start_position = tok_start_position - doc_start + doc_offset\n",
        "                end_position = tok_end_position - doc_start + doc_offset\n",
        "\n",
        "        features.append(\n",
        "            KorQuADFeatures(\n",
        "                span[\"input_ids\"],\n",
        "                span[\"attention_mask\"],\n",
        "                span[\"token_type_ids\"],\n",
        "                cls_index,\n",
        "                p_mask.tolist(),\n",
        "                example_index=0, \n",
        "                unique_id=0,\n",
        "                paragraph_len=span[\"paragraph_len\"],\n",
        "                token_is_max_context=span[\"token_is_max_context\"],\n",
        "                tokens=span[\"tokens\"],\n",
        "                token_to_orig_map=span[\"token_to_orig_map\"],\n",
        "                start_position=start_position,\n",
        "                end_position=end_position,\n",
        "                is_impossible=span_is_impossible,\n",
        "                qas_id=example.qas_id,\n",
        "            )\n",
        "        )\n",
        "    return features"
      ],
      "metadata": {
        "id": "WKqIZ0J5nMwC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## def KorQuAD_convert_examples_to_features"
      ],
      "metadata": {
        "id": "F7vbDNOEnxw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def KorQuAD_convert_example_to_features_init(tokenizer_for_convert):\n",
        "    global tokenizer\n",
        "    tokenizer = tokenizer_for_convert\n",
        "\n",
        "def KorQuAD_convert_examples_to_features(\n",
        "    examples,\n",
        "    tokenizer,\n",
        "    max_seq_length,\n",
        "    doc_stride,\n",
        "    max_query_length,\n",
        "    is_training,\n",
        "):\n",
        "    features = []\n",
        "\n",
        "    # Multiprocessing을 통한 features list 구축\n",
        "    with Pool(os.cpu_count(), initializer=KorQuAD_convert_example_to_features_init, initargs=(tokenizer,)) as p:\n",
        "        annotate_ = partial(\n",
        "            KorQuAD_convert_example_to_features,\n",
        "            max_seq_length=max_seq_length,\n",
        "            doc_stride=doc_stride,\n",
        "            max_query_length=max_query_length,\n",
        "            is_training=is_training,\n",
        "        )\n",
        "        features = list(\n",
        "            tqdm(\n",
        "                p.imap(annotate_, examples, chunksize=32),\n",
        "                total=len(examples),\n",
        "                desc=\"convert KorQuAD examples to features\",\n",
        "            )\n",
        "        )\n",
        "\n",
        "    new_features = []\n",
        "    unique_id = 1000000000\n",
        "    example_index = 0\n",
        "    for example_features in tqdm(features, total=len(features), desc=\"add example index and unique id\"):\n",
        "        if not example_features:\n",
        "            continue\n",
        "        for example_feature in example_features:\n",
        "            example_feature.example_index = example_index\n",
        "            example_feature.unique_id = unique_id\n",
        "            new_features.append(example_feature)\n",
        "            unique_id += 1\n",
        "        example_index += 1\n",
        "    features = new_features\n",
        "    del new_features\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    \"\"\"\n",
        "    [실습]\n",
        "    torch.tensor 자료형의\n",
        "\n",
        "    all_input_ids,\n",
        "    all_attention_masks,\n",
        "    all_token_type_ids,\n",
        "    all_cls_index,\n",
        "    all_p_mask,\n",
        "    all_is_impossible\n",
        "\n",
        "    을 구축하시오.\n",
        "    \"\"\"\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_masks = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n",
        "    all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n",
        "    all_is_impossible = torch.tensor([f.is_impossible for f in features], dtype=torch.float)\n",
        "\n",
        "    if not is_training:\n",
        "        all_feature_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids, all_attention_masks, all_token_type_ids, all_feature_index, all_cls_index, all_p_mask)\n",
        "    else:\n",
        "        all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)\n",
        "        all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)\n",
        "        dataset = TensorDataset(\n",
        "            all_input_ids,\n",
        "            all_attention_masks,\n",
        "            all_token_type_ids,\n",
        "            all_start_positions,\n",
        "            all_end_positions,\n",
        "            all_cls_index,\n",
        "            all_p_mask,\n",
        "            all_is_impossible,\n",
        "        )\n",
        "\n",
        "    return features, dataset"
      ],
      "metadata": {
        "id": "IXxegOezmhIJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실습 출력"
      ],
      "metadata": {
        "id": "2d4MFrDfxDmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
        "sample_max_seq_length = 384\n",
        "sample_max_query_length = 64\n",
        "sample_doc_stride = 128\n",
        "\n",
        "sample_features, sample_dataset = KorQuAD_convert_examples_to_features(\n",
        "    examples=[sample_example[1]],\n",
        "    tokenizer=sample_tokenizer,\n",
        "    max_seq_length=sample_max_seq_length,\n",
        "    doc_stride=sample_doc_stride,\n",
        "    max_query_length=sample_max_query_length,\n",
        "    is_training=True,\n",
        ")"
      ],
      "metadata": {
        "id": "rHsWnfX6x5h3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148,
          "referenced_widgets": [
            "f994c6068f0842dc8921ed8760d654f3",
            "f0f9bbac6e314b8f943f6453f5d180bc",
            "984df1e7a9414cc69e2d199216ff118b",
            "e82327f2d6bc45c9b8a8eba6c0d5bec9",
            "ab55b545ea10495193aa8351f7628591",
            "342f1cec10db4908b6714078be808b38",
            "51c2feef3b214379992940698bf79a64",
            "25bae026a59a4fd4a41e62d78b0c15e1",
            "2f038651187641119e86c760f76cdb0d",
            "64744bf751bb40189741289867b19ab6",
            "394cfb4768ad4aebb6746f276f20c09e",
            "2e1fb949accd433382d2741ef61a5f0c",
            "264d30522ad748d5ada136acb3cdc8db",
            "6efdba1b261945e0b10265cc8e6dddd2",
            "6f5cdfbbe6084e72a3703820be488c7d",
            "d3956dfa3b154d958952ef3c8be81dad",
            "5b1d902f9ef448c9a6f9ee940fb3f8d3",
            "97e3b1c8914f478dba0adfbc69e2608c",
            "d664007ed349455db5c2ee4e966786f2",
            "0f97e995f69640ee85e894259c408957",
            "a519f39ab55545c0b36cbccd589c671a",
            "e164748981d7453d82443e748656a128",
            "128b434d66fd45aeb49931f3aee94d30",
            "d4f8c66823be4c7e94ce6ce6b529bdf5",
            "4e2238e765af44cdbba06a015de810a0",
            "541b79a5001446979262759bede8f9d9",
            "7212a38025a24cca881a030e2a328789",
            "1387cb5e60c546bba6387298aca731e2",
            "4e62ad7d2b1748048d55f146938d1ee4",
            "ce3d5d7c7f254930a05af8f19d8ab8ab",
            "1c682d384d1a423bbdbd62bb61b21ae5",
            "6a70c3e2a97d4baab4ff0f1b4cba59d9",
            "336990e050164dd6b0ab3ad68859df57"
          ]
        },
        "outputId": "04ad8db1-3b97-4199-888b-93287afa10c7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f994c6068f0842dc8921ed8760d654f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e1fb949accd433382d2741ef61a5f0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "128b434d66fd45aeb49931f3aee94d30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert KorQuAD examples to features: 100%|██████████| 1/1 [00:00<00:00, 66.63it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 후처리"
      ],
      "metadata": {
        "id": "X63ZBeo-5b53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KorQuADResult:\n",
        "    def __init__(self, unique_id, start_logits, end_logits):\n",
        "        self.start_logits = start_logits\n",
        "        self.end_logits = end_logits\n",
        "        self.unique_id = unique_id"
      ],
      "metadata": {
        "id": "ODDfdnHVa63o"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_best_indexes(logits, n_best_size):\n",
        "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    best_indexes = []\n",
        "    for i in range(len(index_and_score)):\n",
        "        if i >= n_best_size:\n",
        "            break\n",
        "        best_indexes.append(index_and_score[i][0])\n",
        "    return best_indexes\n",
        "\n",
        "def _compute_softmax(scores):\n",
        "    \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
        "    if not scores:\n",
        "        return []\n",
        "\n",
        "    max_score = None\n",
        "    for score in scores:\n",
        "        if max_score is None or score > max_score:\n",
        "            max_score = score\n",
        "\n",
        "    exp_scores = []\n",
        "    total_sum = 0.0\n",
        "    for score in scores:\n",
        "        x = math.exp(score - max_score)\n",
        "        exp_scores.append(x)\n",
        "        total_sum += x\n",
        "\n",
        "    probs = []\n",
        "    for score in exp_scores:\n",
        "        probs.append(score / total_sum)\n",
        "    return probs\n",
        "\n",
        "def get_final_text(pred_text, orig_text, do_lower_case):\n",
        "    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
        "\n",
        "    def _strip_spaces(text):\n",
        "        ns_chars = []\n",
        "        ns_to_s_map = OrderedDict()\n",
        "        for i, c in enumerate(text):\n",
        "            if c == \" \":\n",
        "                continue\n",
        "            ns_to_s_map[len(ns_chars)] = i\n",
        "            ns_chars.append(c)\n",
        "        ns_text = \"\".join(ns_chars)\n",
        "        return (ns_text, ns_to_s_map)\n",
        "\n",
        "    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
        "\n",
        "    start_position = tok_text.find(pred_text)\n",
        "    if start_position == -1:\n",
        "        return orig_text\n",
        "        \n",
        "    end_position = start_position + len(pred_text) - 1\n",
        "\n",
        "    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
        "    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
        "\n",
        "    if len(orig_ns_text) != len(tok_ns_text):\n",
        "        return orig_text\n",
        "\n",
        "    tok_s_to_ns_map = {}\n",
        "    for i, tok_index in tok_ns_to_s_map.items():\n",
        "        tok_s_to_ns_map[tok_index] = i\n",
        "\n",
        "    orig_start_position = None\n",
        "    if start_position in tok_s_to_ns_map:\n",
        "        ns_start_position = tok_s_to_ns_map[start_position]\n",
        "        if ns_start_position in orig_ns_to_s_map:\n",
        "            orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
        "\n",
        "    if orig_start_position is None:\n",
        "        return orig_text\n",
        "\n",
        "    orig_end_position = None\n",
        "    if end_position in tok_s_to_ns_map:\n",
        "        ns_end_position = tok_s_to_ns_map[end_position]\n",
        "        if ns_end_position in orig_ns_to_s_map:\n",
        "            orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
        "\n",
        "    if orig_end_position is None:\n",
        "        return orig_text\n",
        "\n",
        "    output_text = orig_text[orig_start_position : (orig_end_position + 1)]\n",
        "    return output_text\n",
        "\n",
        "\n",
        "def compute_predictions_logits(\n",
        "    all_examples,\n",
        "    all_features,\n",
        "    all_results,\n",
        "    n_best_size,\n",
        "    max_answer_length,\n",
        "    do_lower_case,\n",
        "    output_prediction_file,\n",
        "    output_nbest_file,\n",
        "    tokenizer,\n",
        "):\n",
        "    if output_prediction_file:\n",
        "        logger.info(f\"Writing predictions to: {output_prediction_file}\")\n",
        "    if output_nbest_file:\n",
        "        logger.info(f\"Writing nbest to: {output_nbest_file}\")\n",
        "\n",
        "    example_index_to_features = defaultdict(list)\n",
        "    for feature in all_features:\n",
        "        example_index_to_features[feature.example_index].append(feature)\n",
        "\n",
        "    unique_id_to_result = {}\n",
        "    for result in all_results:\n",
        "        unique_id_to_result[result.unique_id] = result\n",
        "\n",
        "    _PrelimPrediction = namedtuple( \n",
        "        \"PrelimPrediction\", [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"]\n",
        "    )\n",
        "\n",
        "    all_predictions = OrderedDict()\n",
        "    all_nbest_json = OrderedDict()\n",
        "    scores_diff_json = OrderedDict()\n",
        "\n",
        "    for example_index, example in enumerate(all_examples):\n",
        "        features = example_index_to_features[example_index]\n",
        "\n",
        "        prelim_predictions = []\n",
        "        # keep track of the minimum score of null start+end of position 0\n",
        "        score_null = 1000000  # large and positive\n",
        "        min_null_feature_index = 0  # the paragraph slice with min null score\n",
        "        null_start_logit = 0  # the start logit at the slice with min null score\n",
        "        null_end_logit = 0  # the end logit at the slice with min null score\n",
        "        for feature_index, feature in enumerate(features):\n",
        "            result = unique_id_to_result[feature.unique_id]\n",
        "            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
        "            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # We could hypothetically create invalid predictions, e.g., predict\n",
        "                    # that the start of the span is in the question. We throw out all\n",
        "                    # invalid predictions.\n",
        "                    if start_index >= len(feature.tokens):\n",
        "                        continue\n",
        "                    if end_index >= len(feature.tokens):\n",
        "                        continue\n",
        "                    if start_index not in feature.token_to_orig_map:\n",
        "                        continue\n",
        "                    if end_index not in feature.token_to_orig_map:\n",
        "                        continue\n",
        "                    if not feature.token_is_max_context.get(start_index, False):\n",
        "                        continue\n",
        "                    if end_index < start_index:\n",
        "                        continue\n",
        "                    length = end_index - start_index + 1\n",
        "                    if length > max_answer_length:\n",
        "                        continue\n",
        "                    prelim_predictions.append(\n",
        "                        _PrelimPrediction(\n",
        "                            feature_index=feature_index,\n",
        "                            start_index=start_index,\n",
        "                            end_index=end_index,\n",
        "                            start_logit=result.start_logits[start_index],\n",
        "                            end_logit=result.end_logits[end_index],\n",
        "                        )\n",
        "                    )\n",
        "        prelim_predictions = sorted(prelim_predictions, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\n",
        "\n",
        "        _NbestPrediction = namedtuple( \n",
        "            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n",
        "        )\n",
        "\n",
        "        seen_predictions = {}\n",
        "        nbest = []\n",
        "        for pred in prelim_predictions:\n",
        "            if len(nbest) >= n_best_size:\n",
        "                break\n",
        "            feature = features[pred.feature_index]\n",
        "            if pred.start_index > 0:  # this is a non-null prediction\n",
        "                tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n",
        "                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "                orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n",
        "\n",
        "                tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n",
        "\n",
        "                # Clean whitespace\n",
        "                tok_text = tok_text.strip()\n",
        "                tok_text = \" \".join(tok_text.split())\n",
        "                orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "                final_text = get_final_text(tok_text, orig_text, do_lower_case)\n",
        "                if final_text in seen_predictions:\n",
        "                    continue\n",
        "\n",
        "                seen_predictions[final_text] = True\n",
        "            else:\n",
        "                final_text = \"\"\n",
        "                seen_predictions[final_text] = True\n",
        "\n",
        "            nbest.append(_NbestPrediction(text=final_text, start_logit=pred.start_logit, end_logit=pred.end_logit))\n",
        "\n",
        "        if not nbest:\n",
        "            nbest.append(_NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
        "\n",
        "        assert len(nbest) >= 1, \"No valid predictions\"\n",
        "\n",
        "        total_scores = []\n",
        "        best_non_null_entry = None\n",
        "        for entry in nbest:\n",
        "            total_scores.append(entry.start_logit + entry.end_logit)\n",
        "            if not best_non_null_entry:\n",
        "                if entry.text:\n",
        "                    best_non_null_entry = entry\n",
        "\n",
        "        probs = _compute_softmax(total_scores)\n",
        "\n",
        "        nbest_json = []\n",
        "        for i, entry in enumerate(nbest):\n",
        "            output = OrderedDict()\n",
        "            output[\"text\"] = entry.text\n",
        "            output[\"probability\"] = probs[i]\n",
        "            output[\"start_logit\"] = entry.start_logit\n",
        "            output[\"end_logit\"] = entry.end_logit\n",
        "            nbest_json.append(output)\n",
        "\n",
        "        assert len(nbest_json) >= 1, \"No valid predictions\"\n",
        "\n",
        "        score_diff = score_null - best_non_null_entry.start_logit - (best_non_null_entry.end_logit)\n",
        "        scores_diff_json[example.qas_id] = score_diff\n",
        "        all_predictions[example.qas_id] = best_non_null_entry.text\n",
        "            \n",
        "        all_nbest_json[example.qas_id] = nbest_json\n",
        "\n",
        "    if output_prediction_file:\n",
        "        with open(output_prediction_file, \"w\", encoding='utf-8') as writer:\n",
        "            json.dump(all_predictions, writer, indent='\\t', ensure_ascii=False)\n",
        "\n",
        "    if output_nbest_file:\n",
        "        with open(output_nbest_file, \"w\", encoding='utf-8') as writer:\n",
        "            json.dump(all_nbest_json, writer, indent='\\t', ensure_ascii=False)\n",
        "\n",
        "    return all_predictions"
      ],
      "metadata": {
        "id": "qr1qxX4f5eI4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_answer(s):    \n",
        "    def remove_(text):\n",
        "        text = re.sub(\"'\", \" \", text)\n",
        "        text = re.sub('\"', \" \", text)\n",
        "        text = re.sub('《', \" \", text)\n",
        "        text = re.sub('》', \" \", text)\n",
        "        text = re.sub('<', \" \", text)\n",
        "        text = re.sub('>', \" \", text) \n",
        "        text = re.sub('〈', \" \", text)\n",
        "        text = re.sub('〉', \" \", text)   \n",
        "        text = re.sub(\"\\(\", \" \", text)\n",
        "        text = re.sub(\"\\)\", \" \", text)\n",
        "        text = re.sub(\"‘\", \" \", text)\n",
        "        text = re.sub(\"’\", \" \", text)      \n",
        "        return text\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_punc(lower(remove_(s))))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    \"\"\"\n",
        "    [실습]\n",
        "    f1-score를 구하시오.\n",
        "\n",
        "    c.f.\n",
        "    비교 단위: string\n",
        "    precision: (pred & true) / pred\n",
        "    recall: (pred & true) / true\n",
        "    \"\"\"\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "   \n",
        "    prediction_Char = []\n",
        "    for tok in prediction_tokens:\n",
        "        now = [a for a in tok]\n",
        "        prediction_Char.extend(now)\n",
        "        \n",
        "    ground_truth_Char = []\n",
        "    for tok in ground_truth_tokens:\n",
        "        now = [a for a in tok]\n",
        "        ground_truth_Char.extend(now)\n",
        "    \n",
        "    common = Counter(prediction_Char) & Counter(ground_truth_Char)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    \n",
        "    precision = 1.0 * num_same / len(prediction_Char)\n",
        "    recall = 1.0 * num_same / len(ground_truth_Char)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    \n",
        "    return f1\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    \"\"\"\n",
        "    [실습]\n",
        "    EM score를 구하시오.\n",
        "\n",
        "    \"\"\"\n",
        "    em = (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "    return em\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "def KorQuAD_evaluate(examples, predictions):\n",
        "    f1 = exact_match = total = 0\n",
        "    for cnt, example in enumerate(examples):\n",
        "        total += 1\n",
        "        qas_id = example.qas_id\n",
        "        if qas_id not in predictions:\n",
        "            message = 'Unanswered question ' + qas_id + ' will receive score 0.'\n",
        "            print(message, file=sys.stderr)\n",
        "            continue\n",
        "        ground_truths = [answer[\"text\"] for answer in example.answers]\n",
        "        prediction = predictions[qas_id]\n",
        "        exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
        "        f1 += metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n",
        "        if cnt == 0 or qas_id == \"6332405-1-0\":\n",
        "            logger.info(\"ground truths: {}\".format(ground_truths))\n",
        "            logger.info(\"prediction: {}\".format(prediction))\n",
        "            logger.info(\"F1: {:.3f} || EM: {:.3f}\\n\".format(metric_max_over_ground_truths(f1_score, prediction, ground_truths), metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)))\n",
        "\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "\n",
        "    return {'exact_match': exact_match, 'f1': f1}"
      ],
      "metadata": {
        "id": "uuQxwPQ_51g-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 로딩\n",
        "## def load_and_cache_examples\n",
        "> 정형화된 데이터가 저장되어 있는 경우 이를 불러오고,\n",
        "> 그렇지 않을 경우 데이터를 정형화한 후 저장(caching) 및 반환하는 함수"
      ],
      "metadata": {
        "id": "wAPbFy5Tja3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n",
        "    # Load data features from cache or dataset file\n",
        "    input_dir = args.data_dir if args.data_dir else \".\"\n",
        "    cached_features_file = os.path.join(\n",
        "        input_dir,\n",
        "        \"cached_{}_{}_{}\".format(\n",
        "            \"dev\" if evaluate else \"train\",\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            str(args.max_seq_length),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features_and_dataset = torch.load(cached_features_file)\n",
        "        features, dataset, examples = (\n",
        "            features_and_dataset[\"features\"],\n",
        "            features_and_dataset[\"dataset\"],\n",
        "            features_and_dataset[\"examples\"],\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", input_dir)\n",
        "        \"\"\"\n",
        "        [실습]\n",
        "        전처리를 위해 정의한 클래스와 함수를 이용해\n",
        "        example, features, dataset을 구축하시오.\n",
        "        \"\"\"\n",
        "        processor = KorQuADProcessor()\n",
        "        if evaluate:\n",
        "            examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)\n",
        "        else:\n",
        "            examples = processor.get_train_examples(args.data_dir, filename=args.train_file)\n",
        "\n",
        "        features, dataset = KorQuAD_convert_examples_to_features(\n",
        "            examples=examples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=args.max_seq_length,\n",
        "            doc_stride=args.doc_stride,\n",
        "            max_query_length=args.max_query_length,\n",
        "            is_training=not evaluate,\n",
        "        )\n",
        "\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
        "\n",
        "    if output_examples:\n",
        "        return dataset, examples, features\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "liN_hOKhjY-d"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 정의: class BertForKorQuAD"
      ],
      "metadata": {
        "id": "YSHA3SbFZPlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForKorQuAD(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        \"\"\"\n",
        "        [실습]\n",
        "        start_position과 end_position을 학습하는 MLP를 선언하시오.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        start_positions: Optional[torch.Tensor] = None,\n",
        "        end_positions: Optional[torch.Tensor] = None,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "\n",
        "        \"\"\"\n",
        "        [실습]\n",
        "        bert로부터 입력 시퀀스의 모든 토큰에 대한 hidden_state를 받은 후,\n",
        "        MLP를 통해 각 토큰마다 start_logits와 end_logits를 계산하는 모델을 구현하시오.\n",
        "\n",
        "        c.f.\n",
        "        https://huggingface.co/docs/transformers/model_doc/bert\n",
        "        \"\"\"\n",
        "        \n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            \"\"\"\n",
        "            [실습]\n",
        "            loss function을 선언한 후, total_loss를 구하시오.\n",
        "            \"\"\"\n",
        "            \n",
        "\n",
        "        output = (start_logits, end_logits) + outputs[2:]\n",
        "\n",
        "        return ((total_loss,) + output) if total_loss is not None else output"
      ],
      "metadata": {
        "id": "BIQLHNb_ZPOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate 함수 정의"
      ],
      "metadata": {
        "id": "Dn03KmSP2oxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
        "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
        "\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "    all_results = []\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "\n",
        "            feature_indices = batch[3]\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        for i, feature_index in enumerate(feature_indices):\n",
        "            eval_feature = features[feature_index.item()]\n",
        "            unique_id = int(eval_feature.unique_id)\n",
        "\n",
        "            output = [to_list(output[i]) for output in outputs]\n",
        "\n",
        "            start_logits, end_logits = output\n",
        "            result = KorQuADResult(unique_id, start_logits, end_logits)\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "    evalTime = timeit.default_timer() - start_time\n",
        "    logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n",
        "\n",
        "    # Compute predictions\n",
        "    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(prefix))\n",
        "    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
        "\n",
        "    predictions = compute_predictions_logits(\n",
        "        examples,\n",
        "        features,\n",
        "        all_results,\n",
        "        args.n_best_size,\n",
        "        args.max_answer_length,\n",
        "        args.do_lower_case,\n",
        "        output_prediction_file,\n",
        "        output_nbest_file,\n",
        "        tokenizer,\n",
        "    )\n",
        "\n",
        "    # Compute the F1 and exact scores.\n",
        "    results = KorQuAD_evaluate(examples, predictions)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "tkU_EwkX2whS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train 함수 정의"
      ],
      "metadata": {
        "id": "V12B7GCa2lw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args, train_dataset, model, tokenizer):\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. accumulation) = %d\",\n",
        "        args.train_batch_size * args.gradient_accumulation_steps\n",
        "    )\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss = 0.0\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    set_seed(args)\n",
        "    for _ in trange(int(args.num_train_epochs), position=0, desc=\"Epoch...\"):\n",
        "        model.train()\n",
        "        for step, batch in enumerate(tqdm(train_dataloader, position=0, desc=\"Iteration...\")):\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "                \"start_positions\": batch[3],\n",
        "                \"end_positions\": batch[4],\n",
        "            }\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "                \n",
        "            loss.backward()\n",
        "            tr_loss += loss.item()\n",
        "\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  \n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "        # Log metrics and Save model checkpoint!\n",
        "        results = evaluate(args, model, tokenizer)\n",
        "        logger.info(\"***** Evaluation result *****\")\n",
        "        for key, value in results.items():\n",
        "            logger.info(\"eval_{}: {}\".format(key, value))\n",
        "\n",
        "        output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "        os.makedirs(output_dir)\n",
        "        model.save_pretrained(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "        logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "        logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "        \n",
        "\n",
        "    return global_step, tr_loss / global_step"
      ],
      "metadata": {
        "id": "AeEmLfqe2nu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main함수 정의"
      ],
      "metadata": {
        "id": "7uF2Qr4yXYxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n",
        "        logger.warning(\n",
        "            \"WARNING - You've set a doc stride which may be superior to the document length in some \"\n",
        "            \"examples. This could result in errors when building features from the examples. Please reduce the doc \"\n",
        "            \"stride or increase the maximum length to ensure the features are correctly built.\"\n",
        "        )\n",
        "\n",
        "    if (\n",
        "        os.path.exists(args.output_dir)\n",
        "        and os.listdir(args.output_dir)\n",
        "        and args.do_train\n",
        "        and not args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
        "                args.output_dir\n",
        "            )\n",
        "        )\n",
        "    \n",
        "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
        "    logger.warning(\n",
        "        \"Process device: %s, n_gpu: %s\",\n",
        "        args.device,\n",
        "        args.n_gpu,\n",
        "    )\n",
        "\n",
        "    # Set seed\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "    # Load BERT configurations\n",
        "    config = BertConfig.from_pretrained(args.model_name_or_path)\n",
        "    tokenizer = BertTokenizer.from_pretrained(args.model_name_or_path, do_lower_case=args.do_lower_case)\n",
        "    model = BertForKorQuAD.from_pretrained(args.model_name_or_path, config=config)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Train\n",
        "    if args.do_train:\n",
        "        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "        # Save the trained model and the tokenizer\n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "        model.save_pretrained(args.output_dir)\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "        model = BertForKorQuAD.from_pretrained(args.output_dir)\n",
        "        tokenizer = BertTokenizer.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
        "        model.to(args.device)\n",
        "\n",
        "    # Evaluate\n",
        "    results = {}\n",
        "    if args.do_eval:\n",
        "        logger.info(\"Loading checkpoints saved during training for evaluation\")\n",
        "        checkpoints = [args.output_dir]\n",
        "        if args.do_train and args.eval_all_checkpoints:\n",
        "                checkpoints = list(\n",
        "                    os.path.dirname(c)\n",
        "                    for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
        "                )\n",
        "\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            # Reload the model\n",
        "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
        "            model = BertForKorQuAD.from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "\n",
        "            # Evaluate\n",
        "            result = evaluate(args, model, tokenizer, prefix=global_step)\n",
        "\n",
        "            result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "    logger.info(\"Results: {}\".format(results))\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "eceThL3ZXaEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arguments Parsing과 Main 함수 실행"
      ],
      "metadata": {
        "id": "gWBys9IwXFvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Arguments for several paths or file names\n",
        "    parser.add_argument(\n",
        "        \"--model_name_or_path\",\n",
        "        default='bert-base-multilingual-cased',\n",
        "        type=str,\n",
        "        help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--data_dir\",\n",
        "        default=\"./data\",\n",
        "        type=str,\n",
        "        help=\"The input data dir. Should contain the .json files for the task.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        default=\"./outputs/\",\n",
        "        type=str,\n",
        "        help=\"The output directory where the model checkpoints and predictions will be written.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--train_file\",\n",
        "        default=\"KorQuAD_v1.0_train.json\",\n",
        "        type=str,\n",
        "        help=\"The input training file. If a data dir is specified, will look for the file there\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--predict_file\",\n",
        "        default=\"KorQuAD_v1.0_dev.json\",\n",
        "        type=str,\n",
        "        help=\"The input evaluation file. If a data dir is specified, will look for the file there\",\n",
        "    )\n",
        "\n",
        "    # Hyperparameters\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
        "    parser.add_argument(\n",
        "        \"--max_seq_length\",\n",
        "        default=384,\n",
        "        type=int,\n",
        "        help=\"The maximum total input sequence length after WordPiece tokenization. Sequences \"\n",
        "        \"longer than this will be truncated, and sequences shorter than this will be padded.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--doc_stride\",\n",
        "        default=128,\n",
        "        type=int,\n",
        "        help=\"When splitting up a long document into chunks, how much stride to take between chunks.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_query_length\",\n",
        "        default=64,\n",
        "        type=int,\n",
        "        help=\"The maximum number of tokens for the question. Questions longer than this will \"\n",
        "        \"be truncated to this length.\",\n",
        "    )\n",
        "    parser.add_argument(\"--train_batch_size\", default=4, type=int, help=\"Batch size GPU/CPU for training.\")\n",
        "    parser.add_argument(\n",
        "        \"--eval_batch_size\", default=8, type=int, help=\"Batch size GPU/CPU for evaluation.\"\n",
        "    )\n",
        "    parser.add_argument(\"--learning_rate\", default=3e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    )\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\n",
        "        \"--num_train_epochs\", default=1.0, type=float, help=\"Total number of training epochs to perform.\"\n",
        "    )\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "    parser.add_argument(\n",
        "        \"--n_best_size\",\n",
        "        default=20,\n",
        "        type=int,\n",
        "        help=\"The total number of n-best predictions to generate in the nbest_predictions.json output file.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_answer_length\",\n",
        "        default=30,\n",
        "        type=int,\n",
        "        help=\"The maximum length of an answer that can be generated. This is needed because the start \"\n",
        "        \"and end predictions are not conditioned on one another.\",\n",
        "    )\n",
        "\n",
        "    # Actions\n",
        "    parser.add_argument(\"--do_train\", default=False, action=\"store_true\", help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", default=True, action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\n",
        "        \"--do_lower_case\", default=False, action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--eval_all_checkpoints\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
        "    )\n",
        "    parser.add_argument(\"--no_cuda\", default=False, action=\"store_true\", help=\"Whether not to use CUDA when available\")\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_output_dir\", default=True, action=\"store_true\", help=\"Overwrite the content of the output directory\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_cache\", default=False, action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n",
        "    )\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    # Logging hyperparameters\n",
        "    logger.info(\"Training and evaluation parameters\")\n",
        "    for k, v in args.__dict__.items():\n",
        "        logger.info(\"{}: {}\".format(k, v))\n",
        "    \n",
        "    main(args)"
      ],
      "metadata": {
        "id": "U4y_DRefXE37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실습 과제\n",
        "\n",
        "## BERT 기반 MRC 모델을 학습하고 결과를 출력하시오.\n",
        "- 학습 시간이 오래 소요될 수 있으므로 1 epoch만 학습하셔도 됩니다.\n",
        "- 데이터를 정형화한 후 저장할 때 memory 부족 현상이 발생합니다. 해당 현상이 발생하는 경우 KorQuADPreprocessor 클래스의 get_train_examples와 get_dev_examples 함수에서 데이터 개수를 조절해서 학습하시기 바랍니다.\n",
        "- 문의 사항 있으시면 언제든 아래 메일 주소로 메일 주세요.\n",
        "\n",
        "\n",
        "### 제출: bonggeun.choi818@gmail.com\n",
        "### 제출물: 이름.ipynb"
      ],
      "metadata": {
        "id": "MwhR2kegXTzi"
      }
    }
  ]
}